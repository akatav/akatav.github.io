<!DOCTYPE html>
<html lang="en">
	<head>
        <title>Fisher's Discriminant Analysis | timeline</title>
        <meta charset="utf-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
        <meta name="keywords" content="fishers discriminant analysis,dimensionality reduction,classification,pca,R" />
        <meta name="author" content="SVenka" />
        <meta name="description" content="Fishers discriminant analysis" />
        <meta name="viewport" content="width=640px, initial-scale=1.0, maximum-scale=1.0">	
        <!-- favicon -->
        <link rel="shortcut icon" href="/images/favicon.ico">
        <!-- Custom CSS -->
        <link rel="stylesheet" href="/css/main.css" type="text/css" media="screen, projection" />
        <script type="text/javascript" async 
src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?
config=TeX-AMS-MML_HTMLorMML"></script>
	</head>
	<body>

		
<header>
	<h1>
		<a href="/">Fisher's Discriminant Analysis</a>
	</h1>
	<br>
	<small>
		by <a href="http://linkedin.com" target="_blank">Madudambi</a>
	</small>
</header>


		<nav>
	<ol>
		<li>
			<a href="/chapter1.html" class="">Chapter 1
			<span>A&nbsp;simple&nbsp;introduction</span> 
			</a>
		</li>
		<li>
			<a href="/chapter2.html" class="">Chapter 2
			<span>Intuition</span> 
			</a>
		</li>
		<li>
			<a href="/chapter3.html" class="">Chapter 3
			<span>Fisher's&nbsp;discriminant</span> 
			</a>
		</li>
		<li>
			<a href="/chapter4.html" class="active">Chapter 4
			<span>Mathematical&nbsp;derivation</span> 
			</a>
		</li>
		<li>
			<a href="/chapter5.html" class="">Chapter 5
			<span>Existing&nbsp;implementations</span> 
			</a>
		</li>
		<li>
			<a href="/chapter6.html" class="">Chapter 6
			<span>Code&nbsp;review</span> 
			</a>
		</li>
		<li>
			<a href="/chapter9.html" class="">Chapter 7
			<span>Example&nbsp;1</span> 
			</a>
		</li>
		<li>
			<a href="references.html" class="">References
			<span>References</span>
			</a>
		</li>
	</ol>
</nav>
	

<article>

<h1 id="mathematical-derivation-of-fishers-discriminants">Mathematical derivation of Fisher’s discriminant(s)</h1>

<p>Let <script type="math/tex">\mu_g</script> be the mean of <em>X</em>. Let the mean of each of the <em>K</em> groups of data (denoted by <script type="math/tex">X_k</script>) be <script type="math/tex">\mu_k</script>, <script type="math/tex">1 \le k \le K</script>. We aim to find the Fisher’s discriminant <em>w</em> such that <script type="math/tex">Y=w.X</script>. This was explained in the previous chapter.</p>

<p>Let <script type="math/tex">X'</script> be the set of orthogonal projections of <script type="math/tex">X</script> on the discriminant <em>Y</em>.</p>

<script type="math/tex; mode=display">\begin{equation}
\mu_g' = w.\mu_g\tag{1}\label{eq:one}
\end{equation}</script>

<p>be the orthogonal projection of <script type="math/tex">\mu_g</script> on <em>Y</em>.</p>

<p>Similarly,</p>

<script type="math/tex; mode=display">\begin{equation}
\mu_k'=w.\mu_k\tag{2}\label{eq:two}
\end{equation}</script>

<p>be the orthogonal projections of each <script type="math/tex">\mu_k</script> on <em>Y</em>.</p>

<p>As described in the previous chapter, <em>FDA</em> tries to find <em>w</em> by maximizing the distance between the group means while minimizing the distance between points within a group <em>k</em> and their respective means in the projected space <script type="math/tex">d'</script>.</p>

<p>Let <script type="math/tex">S_w'</script> be the <strong>within-class covariance</strong> and <script type="math/tex">S_b'</script> be the <strong>between-class covariance</strong> in the projected space.</p>

<script type="math/tex; mode=display">\begin{equation}
S_w' = \sum_{k=1}^K (\mu_k'-X_k').(\mu_k'-X_k')^T\tag{3}\label{eq:three}
\end{equation}</script>

<script type="math/tex; mode=display">\begin{equation}
S_b' = \sum_{k=1}^K (\mu_k'-\mu_g').(\mu_k'-\mu_g')^T\tag{4}\label{eq:four}
\end{equation}</script>

<p>Note that, <script type="math/tex">S_w'</script> and <script type="math/tex">S_b'</script> represent the covariances within and between the various classes/groups of the data.</p>

<p>We know that covariance of <em>X</em> is:</p>

<script type="math/tex; mode=display">E[X] = E[(X-E(X))(X-E(X))^T]\tag{5}\label{eq:five}</script>

<p>Covariance of <script type="math/tex">Y</script> is:</p>

<script type="math/tex; mode=display">E[w.X] = E[(wX-E(wX))(wX-E(wX))^T]\tag{6}\label{eq:six}</script>

<script type="math/tex; mode=display">E[wX] = E[w(X-E(X))(X-E(X))^Tw^T]</script>

<script type="math/tex; mode=display">E[wX] = w.E[(X-E(X))(X-E(X))^T].w^T</script>

<p>Substituting equation (5),</p>

<script type="math/tex; mode=display">E[wX] = w.E[X].w^T\tag{7}\label{eq:seven}</script>

<p>From equation (7), we can rewrite <script type="math/tex">S_w'</script> and <script type="math/tex">S_b'</script> in terms of <script type="math/tex">S_w</script> and <script type="math/tex">S_b</script> as follows:</p>

<script type="math/tex; mode=display">S_w'=w.S_w.w^T\tag{8}\label{eq:eight}</script>

<script type="math/tex; mode=display">S_b'=w.S_b.w^T\tag{9}\label{eq:nine}</script>

<p>As mentioned, we try to maximize <script type="math/tex">S_b'</script>, which is the distances between all the <script type="math/tex">\mu_k'</script> of each of the <em>K</em> classes. It turns out that this is the same as maximizing the distance from each <script type="math/tex">\mu_k'</script> and the global mean <script type="math/tex">\mu_g'</script>.</p>

<p>The optimization problem is as follows:</p>

<script type="math/tex; mode=display">J(w) = \frac{S_b'}{S_w'}</script>

<script type="math/tex; mode=display">J(w) = \frac{wS_bw^T}{wS_ww^T}</script>

<p>Rewriting the above equation as a Lagrangian, we get:</p>

<script type="math/tex; mode=display">J(w) = wS_bw^T + \lambda (wS_ww^T-1)</script>

<p>Differentiating w.r.t <em>w</em> and setting to 0,</p>

<script type="math/tex; mode=display">\frac{\partial J}{\partial w} = 2wS_b +2 \lambda wS_w</script>

<script type="math/tex; mode=display">wS_b + \lambda wS_w = 0</script>

<script type="math/tex; mode=display">wS_b = -\lambda wS_w\tag{10}\label{eq:ten}</script>

<p><em>w</em> is a <script type="math/tex">[d,1]</script> vector, <script type="math/tex">S_b</script> and <script type="math/tex">S_w</script> are <script type="math/tex">[d,d]</script> matrices. <script type="math/tex">\lambda</script> is a scalar. The above can be rewritten as follows in the form of the <strong>Generalized Eigenvalue problem</strong></p>

<p>Multiply both sides by <script type="math/tex">S_w^{-1}</script>,</p>

<script type="math/tex; mode=display">wS_w^{-1}S_b = -\lambda w\tag{11}\label{eq:eleven}</script>

<p><script type="math/tex">w=eigen(S_w^{-1}S_b)\tag{12}\label{eq:twleve}</script> gives value of the Fisher’s discriminant <em>w</em>.  We are interested in the first <script type="math/tex">min(K-1, d)</script> eigen vectors in descending magnitude (of eigen values).</p>

<p>Multiplying the original data <em>X</em> by <em>w</em> as obtained above gives the orthogonal projection of <em>X</em>.</p>

<h3 id="differences-between-pca-and-fishers">Differences between PCA and Fishers</h3>

<p>We see that Fishers discriminants are found by eigen decomposition of <script type="math/tex">S_w^{-1}S_b</script>. In contrast, PCA find its components by the eigen decomposition of the covariance matrix of <em>X</em>, <script type="math/tex">eigen(S_t)</script> where <script type="math/tex">S_t</script> is the covariance of the entire data, <em>X</em>.</p>

<p>We note here that <script type="math/tex">S_t=S_w+S_b</script>. We will derive this also for the sake of thoroughness.</p>

<p>Thus, we see that PCA and <em>FDA</em> are different in the covariance matrices they decompose. Also, from this, we understand that <em>PCA</em> decomposes the covariance matrix devoid of any class labels unlike <em>FDA</em>.</p>

<h3 id="total-covariance-is-the-sum-of-within-class-and-between-class-covariance">Total covariance is the sum of within-class and between-class covariance</h3>

<script type="math/tex; mode=display">S_t = S_w + S_b</script>

<script type="math/tex; mode=display">S_w = \sum_{k=1}^{K} \sum_{i=1}^{n} (x_i^k-\mu_k)(x_i^k-\mu_k)^T\tag{13}\label{eq:twelve}</script>

<script type="math/tex; mode=display">S_b = \sum_{k=1}^{K} (\mu_k-\mu_g)(\mu_k-\mu_g)^T\tag{14}\label{eq:thirteen}</script>

<script type="math/tex; mode=display">S_t = \sum_{k=1}^K \sum_{i=1}^{n} (x_i^k - \mu_g)(x_i^k - \mu_g)^T\tag{15}\label{eq:fourteen}</script>

<p><script type="math/tex">S_t</script> can be rewritten as below:</p>

<script type="math/tex; mode=display">S_t = \sum_{k=1}^K \sum_{i=1}^{n} (x_i^k-\mu_k+\mu_k-\mu_g)(x_i^k-\mu_k+\mu_k-\mu_g)^T</script>

<script type="math/tex; mode=display">S_t = \sum_{k=1}^K \sum_{i=1}^{n} (x_i^k - \mu_k)(x_i^k-\mu_k)^T + (\mu_k-\mu_g)(\mu_k-\mu_g)^T
\\ + (x_i^k - \mu_k)(\mu_k-\mu_g)^T + (\mu_k-\mu_g)(x_i^k - \mu_k)^T</script>

<p>Substitute (14) and (15), we get:</p>

<script type="math/tex; mode=display">S_t = S_w + S_b + \sum_{k=1}^K \sum_{i=1}^{n} (x_i^k - \mu_k)(\mu_k-\mu_g)^T 
\\	+ (\mu_k-\mu_g)(x_i^k - \mu_k)^T\tag{15}</script>

<p>Note that,</p>

<p><script type="math/tex">\sum_{k=1}^K \sum_{i=1}^{n} (x_i^k - \mu_k) = 0</script>.</p>

<p>This is because, sum of the differences of the samples around respective mean is always 0.</p>

<p>Substituting in (15), we get:</p>

<script type="math/tex; mode=display">S_t = S_w + S_b</script>

<p><a class="continue" href="chapter5.html">Next chapter</a></p>



</article>


<footer>
	<p>	Fisher's Discriminant Analysis <br>
	<small>
	&copy; <a href="www.linkedin.com" target="_blank">Madudambi</a>. &nbsp;&nbsp;Some rights reserved.
	</small></p>
</footer>

	



	</body>
</html>
