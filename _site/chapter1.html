<!DOCTYPE html>
<html lang="en">
	<head>
        <title>Fisher's Discriminant Analysis | introduction</title>
        <meta charset="utf-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
        <meta name="keywords" content="pls, partial least squares, path modeling, plspm, pls regression, herman wold, svante wold, saga pls" />
        <meta name="author" content="Gaston Sanchez" />
        <meta name="description" content="The Saga of PLS" />
        <meta name="viewport" content="width=640px, initial-scale=1.0, maximum-scale=1.0">	
        <!-- favicon -->
        <link rel="shortcut icon" href="/images/favicon.ico">
        <!-- Custom CSS -->
        <link rel="stylesheet" href="/css/main.css" type="text/css" media="screen, projection" />
        <script type="text/javascript" async 
src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?
config=TeX-AMS-MML_HTMLorMML"></script>
	</head>
	<body>

		
<header>
	<h1>
		<a href="/">Fisher's Discriminant Analysis</a>
	</h1>
	<br>
	<small>
		by <a href="http://linkedin.com" target="_blank">SVenka</a>
	</small>
</header>


		<nav>
	<ol>
		<li>
			<a href="chapter1.html" class="active">Chapter 1
			<span>A&nbsp;simple&nbsp;introduction</span> 
			</a>
		</li>
		<li>
			<a href="chapter2.html" class="">Chapter 2
			<span>Intuition</span> 
			</a>
		</li>
		<li>
			<a href="chapter3.html" class="">Chapter 3
			<span>Fisher's&nbsp;discriminant</span> 
			</a>
		</li>
		<li>
			<a href="chapter4.html" class="">Chapter 4
			<span>Mathematical&nbsp;derivation</span> 
			</a>
		</li>
		<li>
			<a href="chapter5.html" class="">Chapter 5
			<span>Existing&nbsp;implementations</span> 
			</a>
		</li>
		<li>
			<a href="chapter6.html" class="">Chapter 6
			<span>Code&nbsp;review</span> 
			</a>
		</li>
		<li>
			<a href="chapter7.html" class="">Chapter 7
			<span>Example&nbsp;1</span> 
			</a>
		</li>
		<li>
			<a href="chapter8.html" class="">Chapter 8
			<span>Example&nbsp;2</span> 
			</a>
		</li>
		<li>
			<a href="chapter9.html" class="">Chapter 9
			<span>Some&nbsp;concluding&nbsp;remarks</span> 
			</a>
		</li>
		
	</ol>
</nav>
	

<article>

<h1 id="introduction">Introduction</h1>

<p>Fisher’s discriminant analysis was first introduced by <a href="references.html/Fisher1936">Fisher, 1936</a>. Fisher’s discriminant analysis is a supervised data analysis technique. This means that it is mandatory that the (training) data samples that are to be classified need to be labelled as belonging to one of the output classes. Fisher’s Discriminant Analysis (hitherto referred to as <em>FDA</em>) is also a  dimensionality reduction method in statistical data analysis. This is helpful when the data resides in higher dimensions and needs to be projected onto a lower dimension for easy analysis. In this article, we aim to discuss FDA in detail, both mathematically and implementation-wise.</p>

<blockquote>
  <p>Given dataset <em>X</em> in <em>d</em> dimensions where each data sample <script type="math/tex">x_i</script> is assigned to one of <em>K</em> discrete output class labels. <em>FDA</em> aims to find a <em>discriminant</em> in a lower dimension <script type="math/tex">% <![CDATA[
d' < d %]]></script> onto which <em>X</em> is projected. The projected data points belonging to the <em>K</em> classes are easily seperated on this discriminant.</p>
</blockquote>

<h3 id="differences-between-fishers-discriminant-analysis-and-pca">Differences between Fisher’s Discriminant analysis and PCA</h3>

<p><em>PCA</em> is Principal Component Analysis. Like <em>FDA</em>, <em>PCA</em> is also a dimensionality reduction technique. It aims to find the directions (or principal components) along which data varies the most. It does not, however, require that the data be labelled like <em>FDA</em>. Although, this is beyond  the scope of this article, it is important to draw a contrast between <em>FDA</em> and <em>PCA</em> as both these techniques use common techniques from linear algebra. The main difference between <em>PCA</em> and <em>FDA</em> is one of maximum variance and maximum seperability respectively. Let us use a picture aid to understand this briefly.</p>

<p>Let’s plot the <em>IRIS</em> data. Although for purposes of illustration, we will consider only 2 features (<em>petal length</em>, <em>sepal width</em>) and 2 categories (output labels) namely, <em>setosa</em> and <em>versicolor</em> from the original data.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">library</span><span class="p">(</span><span class="n">ggbiplot</span><span class="p">)</span><span class="w">
</span><span class="n">data</span><span class="o">=</span><span class="n">iris</span><span class="p">[</span><span class="o">!</span><span class="p">(</span><span class="n">iris</span><span class="o">$</span><span class="n">Species</span><span class="o">==</span><span class="s2">"versicolor"</span><span class="p">),]</span><span class="w"> </span><span class="c1">#without setosa</span><span class="w">
</span><span class="n">pca.obj</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">prcomp</span><span class="p">(</span><span class="n">data</span><span class="p">[,</span><span class="nf">c</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="m">4</span><span class="p">)],</span><span class="n">center</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">,</span><span class="n">scale.</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span><span class="n">pca.df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">Species</span><span class="o">=</span><span class="n">data</span><span class="o">$</span><span class="n">Species</span><span class="p">,</span><span class="w"> </span><span class="n">as.data.frame</span><span class="p">(</span><span class="n">pca.obj</span><span class="o">$</span><span class="n">x</span><span class="p">))</span><span class="w">
</span><span class="n">P</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggbiplot</span><span class="p">(</span><span class="n">pca.obj</span><span class="p">,</span><span class="n">obs.scale</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="n">var.scale</span><span class="o">=</span><span class="m">1</span><span class="p">,</span><span class="n">ellipse</span><span class="o">=</span><span class="nb">T</span><span class="p">,</span><span class="n">circle</span><span class="o">=</span><span class="nb">F</span><span class="p">,</span><span class="n">varname.size</span><span class="o">=</span><span class="m">3</span><span class="p">,</span><span class="n">groups</span><span class="o">=</span><span class="n">data</span><span class="o">$</span><span class="n">Species</span><span class="p">)</span><span class="w">
</span><span class="n">P</span></code></pre></figure>

<p><img src="/images/Fig1.png" alt="PCA-FDA" /></p>

<p>Note that, PCA find the directions along which the data varies the most indicated by the 2 brown arrows. However, <em>FDA</em> tries to <em>find</em> the line (or discriminant) given by the black arrow. Projecting the data points onto the black arrow seperates the original data well. Also, note that <em>PCA</em> does <em>not</em> require labelled data. It is a dimensionality reduction method that looks at the entire data and find outs the directions of maximum variance.</p>

<p>We shall come back to the technical differences between the two methods again in Chapters 5 and 6.</p>

<h3 id="core-idea-and-intuition">Core idea and intuition</h3>

<p>If there are such discriminant(s) that provide good seperability to the data after projection, then such discriminant(s) will maximize the distance between each group of points while making each individual group as compact as possible. Maximizing seperatedness between different groups of points of different classes is akin to maximizing the distances between the mean of each group of points. Making each group of points compact is akin to minimizing the euclidean distances between points in a group and their respective mean. This is the core idea behind finding a suitable projection of points on Fisher’s discriminant(s).</p>

<p><a class="continue" href="chapter2.html">Next chapter</a></p>



</article>


<footer>
	<p>	Fisher's Discriminant Analysis <br>
	<small>
	&copy; <a href="www.linkedin.com" target="_blank">SVenka</a>. &nbsp;&nbsp;Some rights reserved.
	</small></p>
</footer>

	



	</body>
</html>
